{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the existing literature on the corporate digit...</td>\n",
       "      <td>Context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thus, to close the aforementioned gap, this pa...</td>\n",
       "      <td>Context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hence, the objective of\\r\\nthis research is to...</td>\n",
       "      <td>Context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The main idea behind the\\r\\ntheory of the diff...</td>\n",
       "      <td>Key insights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the development of this process,\\r\\nthere i...</td>\n",
       "      <td>Key insights</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        labels\n",
       "0  the existing literature on the corporate digit...       Context\n",
       "1  Thus, to close the aforementioned gap, this pa...       Context\n",
       "2  Hence, the objective of\\r\\nthis research is to...       Context\n",
       "3  The main idea behind the\\r\\ntheory of the diff...  Key insights\n",
       "4  In the development of this process,\\r\\nthere i...  Key insights"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_training_data.csv\")\n",
    "df = df[[\"line\", \"label\"]]\n",
    "df.columns = [\"text\", \"labels\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unknown         2961\n",
       "Key findings     965\n",
       "Key insights     786\n",
       "Context          461\n",
       "Definitions      400\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_training_data.csv\")\n",
    "df = df[[\"line\", \"label\"]]\n",
    "df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "# Do stratified train test split using sklearn train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"labels\"])\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Key findings', 'Definitions', 'Unknown', 'Context',\n",
       "       'Key insights'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_df.labels), y = train_df.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Context', 'Definitions', 'Key findings', 'Key insights',\n",
       "        'Unknown'], dtype=object),\n",
       " [2.4162601626016262,\n",
       "  2.78625,\n",
       "  1.154922279792746,\n",
       "  1.4174880763116058,\n",
       "  0.3765202702702703])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_df.labels), list(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcccfea7ff24a29a608ebf832451c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4458.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score,precision_score, recall_score\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs()\n",
    "model_args.num_train_epochs=1\n",
    "model_args.use_multiprocessing = False,\n",
    "#model_args.use_multiprocessing_for_evaluation = False\n",
    "#model_args.use_multiprocessed_decoding = False\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.save_steps = -1\n",
    "model_args.save_model_every_epoch = False\n",
    "#model.args.metric_fnc = f1_multiclass\n",
    "#model_args.wandb_project = \"text_highlight5\"\n",
    "model_args.labels_list = ['Context', 'Definitions', 'Key findings', 'Key insights', 'Unknown']\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args, use_cuda=True, num_labels = 5, weight = list(class_weights)\n",
    ")\n",
    "\n",
    "# Define weighted f1 score function\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "\n",
    "# Define weighted recall score function\n",
    "def macro_recall(labels, preds):\n",
    "    return recall_score(labels, preds, average='weighted')\n",
    "\n",
    "# # Define weighted precision score function\n",
    "def get_precision(labels, preds):\n",
    "    return precision_score(labels, preds, average='weighted')\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df,   f1=f1_multiclass, \n",
    "                                    acc=accuracy_score, \n",
    "                                    recall = macro_recall,\n",
    "                                    precision = get_precision)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "\n",
    "# Make predictions with the model\n",
    "predictions, raw_outputs = model.predict([\"Sam was a Wizard\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO:simpletransformers.classification.classification_model:{'mcc': 0.6049468970115803, 'eval_loss': 0.10204083122796594}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snbha\\AppData\\Roaming\\Python\\Python38\\site-packages\\simpletransformers\\classification\\classification_model.py:496: UserWarning: wandb_project specified but wandb is not available. Wandb disabled.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "saved_model = ClassificationModel(\n",
    "    \"roberta\", \"use_this_model\", use_cuda=False, num_labels = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = saved_model.predict([\"Sam was a Wizard\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unknown']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = saved_model.predict([\"the existing literature on the corporate digital divide does not explicitly identify the accumulation of personal digital competencies that provide for the implementation and use of effective DTs \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unknown']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = saved_model.predict([\"the existing literature on the corporate digital divide does not explicitly identify the accumulation of personal digital competencies that provide for the implementation and use of effective DTs \", \"Apple is tech company\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unknown', 'Unknown']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.54518843, -0.5448392 , -0.3685222 , -2.11993289, -3.09125853],\n",
       "       [ 5.7347312 , -0.57513279, -0.42800692, -2.13170743, -3.12520361]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e588a23f49329b9290dda077d79a9005879a30fe97c1fa2c6a93e79fd5926943"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
